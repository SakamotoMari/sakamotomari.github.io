{"title":"Writing Exercise: Reward Systems in Human Computation Games","date":"2018-10-13T19:06:53.000Z","slug":"Writing-exercise-Reward-Systems-in-Human-Computation-Games","comments":true,"tags":["Class","Exercise","HCI"],"categories":["Notes"],"updated":"2019-04-28T14:57:04.017Z","content":"<h1 id=\"Introduction\">Introduction<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Introduction\"></a></h1><p>A writing exercise based on an IXSci class about experimental design. The original paper is called <em>Reward Systems in Human Computation Games</em>, written by Kristin Siu and Mark O. Riedl. A summary (word limit 500) and a research proposal would be posted here. First draft, without refinement.</p>\n<h1 id=\"Summary\">Summary<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Summary\"></a></h1><p>Human Computation Games (HCG) could support research through requiring players to solve problems that are intractable for computers, while they are not widely adopted due to the difficulty in design and development of effective HCG, which respects both the task completion and the player experience. In mainstream digital games, multiple reward systems are applied to increase motivation and engagement. However, current HCGs tend to apply simple reward systems. In order to clarify the possible influence of multiple reward systems on HCGs, Siu and Riedl investigated the question that whether offering a choice between different reward systems to players would affect the task completion and the player experience. They also attempted to understand how different reward systems influence different audiences of players.</p>\n<p>The researchers conducted a 2 × 2 between-subjects experiment to test their hypotheses. They assumed that offering a choice of rewards to players would lead to longer time of participation, faster operations and better quality of work, compared with randomly distributing rewards. A culinary-themed HCG called Café Flour Sack was built, in which players have to classify cooking ingredients for potential recipes. Four reward systems were incorporated: global leaderboards, customizable avatars, unlockable narratives, and a global progress tracker. </p>\n<p>Two groups of participants were recruited. The first group is familiar with crowdsourcing platforms, which are frequently used to distribute HCGs. The second group is students who tend to be familiar with games but unfamiliar with crowdsourcing. The type of players is the first independent variable in this experiment.  The other manipulated factor is the game condition. Two versions of games were distributed to the players, random and choice. In the choice version, players are allowed to manually select one of the three categories at the beginning of each round. The only difference between the two versions is the reward system selection screen.</p>\n<p>Three metrics were used to evaluate the task completion: the answer correctness, the number of tasks completed and the timing of task completion. The results showed that players in the choice condition had faster mean times for task completion than players in the random condition; the group that are familiar with crowdsourcing was significantly faster than the other group. These findings were also observed for tasks answered correctly and incorrectly. In terms of the player experience, there were no significant differences between each condition, except for a higher perception of reward choice for the players who were allowed to choose a reward and the players that were familiar with crowdsourcing.</p>\n<p>Based on the results, the researchers infer that the introduction of multiple reward systems could make HCGs both effective and engaging if players are allowed to choose rewards.  Furthermore, the adjustment in reward mechanics could improve the performance of certain player audiences.  In conclusion, the study helps to confirm the positive influence of reward mechanics to both the task completion and the player experience.</p>\n<h1 id=\"Proposal\">Proposal<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Proposal\"></a></h1><h2 id=\"Introduction-1\">Introduction<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Introduction-1\"></a></h2><h2 id=\"Related-Work\">Related Work<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Related-Work\"></a></h2><h2 id=\"Method\">Method<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Method\"></a></h2><h3 id=\"Participants\">Participants<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Participants\"></a></h3><p>Participants would be recruited from two populations, each accounting for approximately 40. The first will be through Amazon Mechanical Turk, a paid crowdsourcing platform which has been frequently used for distributing Human Computation Games. The second group will be recruited through a class, such as an undergraduate computer science class, where the students tend to be familiar with games.</p>\n<h3 id=\"Design\">Design<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Design\"></a></h3><p>The experiment is a 2×2 between-subject design. One independent variable is the type of audience, one being familiar with crowdsourcing, the other being not familiar with that. The second independent variable is the game condition. The game has two versions, random and choice. For the random version, players are automatically assigned one of the three reward categories at the beginning of each round. In the choice version, the player is allowed to manually select one of the three categories. During the game session, </p>\n<h3 id=\"Materials\">Materials<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Materials\"></a></h3><h3 id=\"Procedure\">Procedure<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Procedure\"></a></h3><h2 id=\"Data-Analysis-Plan\">Data Analysis Plan<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Data-Analysis-Plan\"></a></h2><h2 id=\"Discussion\">Discussion<a href=\"post/Writing-exercise-Reward-Systems-in-Human-Computation-Games#Discussion\"></a></h2>","prev":{"title":"When a Game is Not a Game","slug":"When-a-Game-is-Not-a-Game"},"next":{"title":"A Failure of Digital Story","slug":"A-Failure-of-Digital-Story"},"link":"https://sakamotomari.github.io/post/Writing-exercise-Reward-Systems-in-Human-Computation-Games/","toc":[{"title":"Introduction","id":"Introduction","index":"1"},{"title":"Summary","id":"Summary","index":"2"},{"title":"Proposal","id":"Proposal","index":"3","children":[{"title":"Method","id":"Method","index":"3.1","children":[{"title":"Participants","id":"Participants","index":"3.1.1"},{"title":"Design","id":"Design","index":"3.1.2"},{"title":"Procedure","id":"Procedure","index":"3.1.3"}]},{"title":"Discussion","id":"Discussion","index":"3.2"}]}]}